# Model Configuration
model:
  name: "Hello-SimpleAI/chatgpt-detector-roberta"  # Can switch to roberta-base for better performance
  num_labels: 2  # Human, AI, Inconclusive
  max_length: 256
  use_hybrid_features: false  # Set to true to add perplexity + stylometric features

# Training Configuration
training:
  output_dir: "./models/checkpoints"
  num_epochs: 2
  batch_size: 4  # Increased for M1 efficiency
  learning_rate: 0.00002  # 2e-5 written as decimal
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 25
  eval_steps: 50   # Evaluate every 50 steps  
  save_steps: 50   # Save checkpoint every 50 steps (much more frequent!)
  save_total_limit: 5  # Keep more checkpoints since we're saving frequently
  gradient_accumulation_steps: 8
  fp16: false  # M1 Macs don't support FP16 well with MPS
  seed: 42

# Data Configuration
data:
  train_path: "./data/splits/train.csv"
  val_path: "./data/splits/val.csv"
  test_path: "./data/splits/test.csv"
  text_column: "text"
  label_column: "label"
  # Label mapping: 0=Human, 1=AI, 2=Inconclusive

# Evaluation Configuration
evaluation:
  metrics:
    - accuracy
    - f1_macro
    - roc_auc
    - calibration_error
  confidence_threshold: 0.7  # Below this, classify as "Inconclusive"

# Hybrid Features (optional)
hybrid_features:
  perplexity_model: "gpt2"
  stylometric_features:
    - type_token_ratio
    - avg_sentence_length
    - lexical_diversity
    - punctuation_ratio
